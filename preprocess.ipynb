{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random\n",
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio import transforms\n",
    "from torchvision import transforms as ts\n",
    "\n",
    "from IPython.display import Audio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "\n",
    "class AudioUtil():\n",
    "  # Load an audio file. Return the signal as a tensor and the sample rate\n",
    "  # deafult setting for sr = 16000\n",
    "  @staticmethod\n",
    "  def open(audio_file):\n",
    "    sig, sr = torchaudio.load(audio_file)\n",
    "    return (sig, sr)\n",
    "\n",
    "  # ----------------------------\n",
    "  # Convert the given audio to the desired number of channels\n",
    "  # stereo = 2, new_channel = 2\n",
    "  # mono = 1, new_channel = 1\n",
    "  @staticmethod\n",
    "  def rechannel(aud, new_channel):\n",
    "    sig, sr = aud\n",
    "\n",
    "    if (sig.shape[0] == new_channel):\n",
    "      # Nothing to do\n",
    "      return aud\n",
    "\n",
    "    if (new_channel == 1):\n",
    "      # Convert from stereo to mono by selecting only the first channel\n",
    "      resig = sig[:1, :]\n",
    "    else:\n",
    "      # Convert from mono to stereo by duplicating the first channel\n",
    "      resig = torch.cat([sig, sig, sig])\n",
    "    \n",
    "    return (resig, sr)\n",
    "  \n",
    "  # Since Resample applies to a single channel, we resample one channel at a time\n",
    "  # need to know what is the optimal value for human voice\n",
    "  # ----------------------------\n",
    "  @staticmethod\n",
    "  def resample(aud, newsr):\n",
    "    sig, sr = aud\n",
    "\n",
    "    if (sr == newsr):\n",
    "      # Nothing to do\n",
    "      return aud\n",
    "\n",
    "    num_channels = sig.shape[0]\n",
    "    # Resample first channel\n",
    "    resig = torchaudio.transforms.Resample(sr, newsr)(sig[:1,:])\n",
    "    if (num_channels > 1):\n",
    "      # Resample the second channel and merge both channels\n",
    "      retwo = torchaudio.transforms.Resample(sr, newsr)(sig[1:,:])\n",
    "      resig = torch.cat([resig, retwo])\n",
    "\n",
    "    return ((resig, newsr))\n",
    "  # ----------------------------\n",
    "  # Pad (or truncate) the signal to a fixed length 'max_ms' in milliseconds\n",
    "  # This is for regular procedure.\n",
    "  # not preferred since no silence included from competition audio files\n",
    "  # ----------------------------\n",
    "  @staticmethod\n",
    "  def pad_trunc(aud, max_ms):\n",
    "    sig, sr = aud\n",
    "    num_rows, sig_len = sig.shape\n",
    "    max_len = sr//1000 * max_ms\n",
    "    if (sig_len > max_len):\n",
    "      direction = random.randint(0,1)\n",
    "      # Truncate the signal to the given length, just cut to the direction.\n",
    "      if direction == 0: # left\n",
    "        sig = sig[:,:max_len]\n",
    "      else: # right\n",
    "        sig = sig[:,sig_len-max_len:]  \n",
    "    elif (sig_len < max_len):\n",
    "      # Length of padding to add at the beginning and end of the signal\n",
    "      pad_begin_len = random.randint(0, max_len - sig_len)\n",
    "      pad_end_len = max_len - sig_len - pad_begin_len\n",
    "      \n",
    "      # Pad with 0s\n",
    "      pad_begin = torch.zeros((num_rows, pad_begin_len))\n",
    "      pad_end = torch.zeros((num_rows, pad_end_len))\n",
    "      \n",
    "      sig = torch.cat((pad_begin, sig, pad_end), 1)\n",
    "      \n",
    "    return (sig, sr)\n",
    "  # ----------------------------\n",
    "  # This applies for our purpose, time shift do not needed for this.\n",
    "  # ----------------------------\n",
    "  @staticmethod\n",
    "  def time_sample(aud, max_ms):\n",
    "    sig, sr = aud\n",
    "    num_rows, sig_len = sig.shape\n",
    "    max_len = sr//1000 * max_ms\n",
    "    if sig[1].shape[0] > max_len:\n",
    "      start = np.random.randint(0,sig[1].shape[0]-max_len)\n",
    "      sam_sig = sig[:,start:start+max_len-1]\n",
    "      return (sam_sig, sr)\n",
    "    else:\n",
    "      con_sig = torch.cat((sig,sig), axis=1)\n",
    "      while con_sig[1].shape[0] < max_len:\n",
    "        con_sig = torch.cat((con_sig,sig), axis=1)\n",
    "      start = np.random.randint(0,con_sig[1].shape[0]-max_len)\n",
    "      con_sig = con_sig[:, start:start+max_len]\n",
    "      return (con_sig, sr)  \n",
    "  ###############################################################################\n",
    "  # Raw Augmentation\n",
    "  ###############################################################################\n",
    "  # ----------------------------\n",
    "  # noise, flip, others are implemented in torchaudio transformations.   \n",
    "  # noise_factor < 0.005 is preferred -> heuristic test\n",
    "  # ----------------------------\n",
    "  @staticmethod\n",
    "  def noise(aud, noise_factor):\n",
    "    sig, sr = aud\n",
    "    # sampling from gaussian distribution\n",
    "    noise = np.random.randn((sig.shape[1]))\n",
    "    num_channels = sig.shape[0]\n",
    "    noise_data = sig[0] + noise_factor * noise\n",
    "    if (num_channels > 1):\n",
    "        noise_data_two =sig[1] + noise_factor*noise\n",
    "        noise_data_r = torch.stack([noise_data, noise_data_two])\n",
    "    else:\n",
    "        noise_data_r = noise_data.reshape(1,-1)  \n",
    "    \n",
    "    return (noise_data_r, sr)\n",
    "  \n",
    "  @staticmethod\n",
    "  def flip(aud):\n",
    "    # if we consider it as speaking out different languages, this might be helpful.\n",
    "    sig, sr = aud\n",
    "    flip_sig = np.flip(sig.numpy()).copy()\n",
    "    \n",
    "    return torch.from_numpy(flip_sig), sr\n",
    "\n",
    "  # Spectrogram\n",
    "  # ----------------------------\n",
    "  # Generate a Spectrogram , mel_spectrogram + dB\n",
    "  # ----------------------------\n",
    "  @staticmethod\n",
    "  def spectro_gram(aud, n_mels=128, n_fft=1024, win_len = None, hop_len=512):\n",
    "    sig,sr = aud\n",
    "    top_db = 80\n",
    "\n",
    "    # spec has shape [channel, n_mels, time], where channel is mono, stereo etc\n",
    "    spec = transforms.MelSpectrogram(sr, n_fft=n_fft, win_length = win_len, hop_length=hop_len, center = True, onesided= True, n_mels=n_mels, mel_scale = 'htk')(sig)\n",
    "        \n",
    "    # Convert to decibels\n",
    "    spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n",
    "    \n",
    "    return (spec)\n",
    "  # ----------------------------\n",
    "  # Generate MFCC\n",
    "  # ----------------------------\n",
    "  @staticmethod\n",
    "  def mfcc(aud, n_mels=128, n_mfcc=20,n_fft=2048, hop_len =512):\n",
    "    sig, sr = aud\n",
    "    \n",
    "    spec = transforms.MFCC(sr, n_mfcc, melkwargs={'n_fft': n_fft, 'n_mels': n_mels, 'hop_length': hop_len, 'mel_scale': 'htk'})(sig)\n",
    "    \n",
    "    return spec\n",
    "  \n",
    "  # Spectrogram Augmentation \n",
    "  \n",
    "  # ----------------------------\n",
    "  # Augment the Spectrogram by masking out some sections of it in both the frequency\n",
    "  # dimension (ie. horizontal bars) and the time dimension (vertical bars) to prevent\n",
    "  # overfitting and to help the model generalise better. The masked sections are\n",
    "  # replaced with the mean value.\n",
    "  # ----------------------------\n",
    "  @staticmethod\n",
    "  def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n",
    "    _, n_mels, n_steps = spec.shape\n",
    "    mask_value = spec.mean()\n",
    "    aug_spec = spec\n",
    "    \n",
    "    freq_mask_param = max_mask_pct * n_mels\n",
    "    for _ in range(n_freq_masks):\n",
    "      aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n",
    "\n",
    "    time_mask_param = max_mask_pct * n_steps\n",
    "    for _ in range(n_time_masks):\n",
    "      aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n",
    "    return aug_spec\n",
    "  \n",
    "  #@staticmethod\n",
    "  # suggest rate btw [0.8, 1.2]\n",
    " # def time_stretch(spec, rate=0.8,hop_len =512, n_freq=128):\n",
    "    \n",
    "  #  stretch = torchaudio.transforms.TimeStretch(hop_length =hop_len, n_freq = n_freq, fixed_rate = rate)\n",
    "  #  streched = stretch(spec, 1.2)\n",
    "  #  return streched\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_hot_encoding\n",
    "def to_one_hot(k, classes_num):\n",
    "    target = np.zeros(classes_num)\n",
    "    target[k] = 1\n",
    "    return target\n",
    "\n",
    "# csv\n",
    "def data_to_frame(data_dir='./dataset/train', min_files=5):\n",
    "    data_path =[]\n",
    "    data_label = []\n",
    "    encoded_label = {}\n",
    "    total_len = len(os.listdir(data_dir))\n",
    "    \n",
    "    k=0\n",
    "    for dirname, _, filenames in os.walk(data_dir):\n",
    "        label = dirname.split('/')[-1]\n",
    "        encoded_label[label] = to_one_hot(k,total_len)\n",
    "        k+=1\n",
    "    #only for mac\n",
    "        if filenames[0] == '.DS_Store':\n",
    "            pass\n",
    "        else:\n",
    "            if len(filenames) < min_files:\n",
    "                a = len(filenames)\n",
    "                while a != min_files:\n",
    "                    random_file = filenames[np.random.randint(0,a)]\n",
    "                    filenames.append(random_file)\n",
    "                    a+=1\n",
    "            for filename in filenames:\n",
    "                data_label.append(label)\n",
    "                data_path.append(os.path.join(dirname,filename))\n",
    "\n",
    "# encoded label 어떻게 처릴할지 생각해야함.\n",
    "# validation 처리\n",
    "# label은 마지막에 처리\n",
    "    df = pd.DataFrame({'path' : data_path, 'class' : data_label })\n",
    "    \n",
    "    return df, encoded_label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_to_frame(min_files=10)\n",
    "df = train_data[0]\n",
    "encoded_label = train_data[1]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchaudio\n",
    "\n",
    "# ----------------------------\n",
    "# Sound Dataset\n",
    "# ----------------------------\n",
    "class voiceDS(Dataset):\n",
    "  def __init__(self, df_x, df_y, data_type, encode_label, transform = None):\n",
    "    self.df_x = df_x\n",
    "    self.df_y = df_y\n",
    "    self.data_type = str(data_type)\n",
    "    self.encoded_label = encoded_label\n",
    "    # transform\n",
    "    self.transform = transform\n",
    "    # hyperparameters\n",
    "    self.duration = 5000\n",
    "    self.sr = 44100\n",
    "    self.channel = 2\n",
    "        \n",
    "  # ----------------------------\n",
    "  # Number of items in dataset\n",
    "  # ----------------------------\n",
    "  def __len__(self):\n",
    "    return len(self.df_x)    \n",
    "    \n",
    "  # ----------------------------\n",
    "  # Get i'th item in dataset\n",
    "  # ----------------------------\n",
    "  def __getitem__(self, idx):\n",
    "    # Absolute file path of the audio file - concatenate the audio directory with\n",
    "    # the relative path\n",
    "    audio_file = self.df_x.loc[idx, 'path']\n",
    "    # Get the Class ID\n",
    "    class_id = self.df_y.loc[idx, 'class']\n",
    "    class_id = torch.Tensor(encoded_label[class_id])\n",
    "    # open the file\n",
    "    aud = AudioUtil.open(audio_file)\n",
    "    # preprocessing: resample, rechannel, fix time\n",
    "    reaud = AudioUtil.resample(aud, self.sr)\n",
    "    rechan = AudioUtil.rechannel(reaud, self.channel) \n",
    "    dur_aud = AudioUtil.time_sample(rechan, self.duration)\n",
    "    # Convert mel spectrogram -> (num_channels, Mel freq_bands, time_steps) -> later needs to match power of 2.\n",
    "    sgram = AudioUtil.spectro_gram(dur_aud, n_mels=64, n_fft=1024, hop_len=None)\n",
    "    if self.data_type == 'train':\n",
    "    # raw augmentation -> later\n",
    "    # aug = AudioUtil.noise(aud,0.001)\n",
    "    # Mel augmentation\n",
    "        sgram = AudioUtil.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n",
    "    \n",
    "    if self.transform:\n",
    "        sgram = self.transform(sgram)\n",
    "\n",
    "    return sgram, class_id\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
